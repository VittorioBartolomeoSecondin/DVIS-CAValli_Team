{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqzvCluVrzPuzjodXLLhoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VittorioBartolomeoSecondin/DVIS-CAValli_Team/blob/main/Temperatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data exploration and preprocessing"
      ],
      "metadata": {
        "id": "K_OLejOUTNVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries and connecting to Google Drive"
      ],
      "metadata": {
        "id": "fEUxDlj-TJlh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-5lfwtzTD_p",
        "outputId": "93fd26b1-42d9-4de5-e2e7-ccd1b4a00499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "import json\n",
        "import csv\n",
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # turned out to be a good workaround to load a huge amount of data and keep it available"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the data"
      ],
      "metadata": {
        "id": "bwliM6Hp2y8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_files = glob.glob(\"/content/drive/MyDrive/tree_dataset/final_dataset/*.txt\")\n",
        "all_files.sort()\n",
        "print(all_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCGvHGJTVYGo",
        "outputId": "04f65e90-2ee9-49ba-f5dd-d167644f0296"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/tree_dataset/final_dataset/climdiv-tmaxst-v1.0.0-20231106.txt', '/content/drive/MyDrive/tree_dataset/final_dataset/climdiv-tminst-v1.0.0-20231106.txt', '/content/drive/MyDrive/tree_dataset/final_dataset/climdiv-tmpcst-v1.0.0-20231106.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['Code', 'JanF', 'FebF', 'MarF', 'AprF', 'MayF', 'JunF', 'JulF', 'AugF', 'SepF', 'OctF', 'NovF', 'DecF']\n",
        "column_data_types = {column_names[0]: str}\n",
        "column_data_types.update({column_names[i]: float for i in range(1, 13)})\n",
        "max_dataset = pd.read_csv(all_files[0], delimiter = r'\\s+', header = None, names = column_names, dtype = column_data_types)\n",
        "min_dataset = pd.read_csv(all_files[1], delimiter = r'\\s+', header = None, names = column_names, dtype = column_data_types)\n",
        "avg_dataset = pd.read_csv(all_files[2], delimiter = r'\\s+', header = None, names = column_names, dtype = column_data_types)"
      ],
      "metadata": {
        "id": "yp9Kf_QXVtGg"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the datasets for the export"
      ],
      "metadata": {
        "id": "UqJ1Twku21qG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace -99.9 with NULL values"
      ],
      "metadata": {
        "id": "eD6XOIgj3h5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_dataset.replace(-99.9, np.nan, inplace=True)\n",
        "min_dataset.replace(-99.9, np.nan, inplace=True)\n",
        "avg_dataset.replace(-99.9, np.nan, inplace=True)"
      ],
      "metadata": {
        "id": "-RsIGAiRYDtf"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert from F to C"
      ],
      "metadata": {
        "id": "BYQJo2453pJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "months_F = column_names[1:]\n",
        "months_C = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "for idx, month in enumerate(months_C):\n",
        "  max_dataset[month] = ((max_dataset[months_F[idx]] - 32) * 5 / 9).round(1)\n",
        "  min_dataset[month] = ((min_dataset[months_F[idx]] - 32) * 5 / 9).round(1)\n",
        "  avg_dataset[month] = ((avg_dataset[months_F[idx]] - 32) * 5 / 9).round(1)"
      ],
      "metadata": {
        "id": "7jndYgLyZXP_"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract *state_code* and *year* from *Code*"
      ],
      "metadata": {
        "id": "AJnZnRUp3t57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_dataset['state_code'] = max_dataset['Code'].str[:3]\n",
        "max_dataset['year'] = max_dataset['Code'].str[6:]\n",
        "\n",
        "min_dataset['state_code'] = min_dataset['Code'].str[:3]\n",
        "min_dataset['year'] = min_dataset['Code'].str[6:]\n",
        "\n",
        "avg_dataset['state_code'] = avg_dataset['Code'].str[:3]\n",
        "avg_dataset['year'] = avg_dataset['Code'].str[6:]"
      ],
      "metadata": {
        "id": "mMfVFwbvq_vs"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dictionary with *state_code* as key and its corresponding *state* nomenclature as value"
      ],
      "metadata": {
        "id": "ujGmTIP131IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"001 Alabama\n",
        "002 Arizona\n",
        "003 Arkansas\n",
        "004 California\n",
        "005 Colorado\n",
        "006 Connecticut\n",
        "007 Delaware\n",
        "008 Florida\n",
        "009 Georgia\n",
        "010 Idaho\n",
        "011 Illinois\n",
        "012 Indiana\n",
        "013 Iowa\n",
        "014 Kansas\n",
        "015 Kentucky\n",
        "016 Louisiana\n",
        "017 Maine\n",
        "018 Maryland\n",
        "019 Massachusetts\n",
        "020 Michigan\n",
        "021 Minnesota\n",
        "022 Mississippi\n",
        "023 Missouri\n",
        "024 Montana\n",
        "025 Nebraska\n",
        "026 Nevada\n",
        "027 New Hampshire\n",
        "028 New Jersey\n",
        "029 New Mexico\n",
        "030 New York\n",
        "031 North Carolina\n",
        "032 North Dakota\n",
        "033 Ohio\n",
        "034 Oklahoma\n",
        "035 Oregon\n",
        "036 Pennsylvania\n",
        "037 Rhode Island\n",
        "038 South Carolina\n",
        "039 South Dakota\n",
        "040 Tennessee\n",
        "041 Texas\n",
        "042 Utah\n",
        "043 Vermont\n",
        "044 Virginia\n",
        "045 Washington\n",
        "046 West Virginia\n",
        "047 Wisconsin\n",
        "048 Wyoming\n",
        "050 Alaska\n",
        "101 Northeast Region\n",
        "102 East North Central Region\n",
        "103 Central Region\n",
        "104 Southeast Region\n",
        "105 West North Central Region\n",
        "106 South Region\n",
        "107 Southwest Region\n",
        "108 Northwest Region\n",
        "109 West Region\n",
        "110 National (contiguous 48 States)\n",
        "111 Great Plains\n",
        "115 Southern Plains and Gulf Coast\n",
        "120 US Rockies and Westward\n",
        "121 NWS Eastern Region\n",
        "122 NWS Southern Region\n",
        "123 NWS Central Region\n",
        "124 NWS Western Region\n",
        "201 Pacific Northwest Basin\n",
        "202 California River Basin\n",
        "203 Great Basin\n",
        "204 Lower Colorado River Basin\n",
        "205 Upper Colorado River Basin\n",
        "206 Rio Grande River Basin\n",
        "207 Texas Gulf Coast River Basin\n",
        "208 Arkansas-White-Red Basin\n",
        "209 Lower Mississippi River Basin\n",
        "210 Missouri River Basin\n",
        "211 Souris-Red-Rainy Basin\n",
        "212 Upper Mississippi River Basin\n",
        "213 Great Lakes Basin\n",
        "214 Tennessee River Basin\n",
        "215 Ohio River Basin\n",
        "216 South Atlantic-Gulf Basin\n",
        "217 Mid-Atlantic Basin\n",
        "218 New England Basin\n",
        "220 Mississippi River Basin & Tributaties (N. of Memphis, TN)\n",
        "250 Spring Wheat Belt (area weighted)\n",
        "255 Primary Hard Red Winter Wheat Belt (area weighted)\n",
        "256 Winter Wheat Belt (area weighted)\n",
        "260 Primary Corn and Soybean Belt (area weighted)\n",
        "261 Corn Belt (area weighted)\n",
        "262 Soybean Belt (area weighted)\n",
        "265 Cotton Belt (area weighted)\n",
        "350 Spring Wheat Belt (productivity weighted)\n",
        "356 Winter Wheat Belt (productivity weighted)\n",
        "361 Corn Belt (productivity weighted)\n",
        "362 Soybean Belt (productivity weighted)\n",
        "365 Cotton Belt (productivity weighted)\n",
        "450 Spring Wheat Belt (% productivity in the Palmer Z Index)\n",
        "456 Winter Wheat Belt (% productivity in the Palmer Z Index)\n",
        "461 Corn Belt (% productivity in the Palmer Z Index)\n",
        "462 Soybean Belt (% productivity in the Palmer Z Index)\n",
        "465 Cotton Belt (% productivity in the Palmer Z Index)\"\"\"\n",
        "\n",
        "lines = data.split('\\n')\n",
        "code_to_state = {line[:3]: line[4:].strip() for line in lines}\n",
        "print(code_to_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HNBTVLTng_q",
        "outputId": "bf0cd633-af95-455d-f745-39e5d2298d0b"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'001': 'Alabama', '002': 'Arizona', '003': 'Arkansas', '004': 'California', '005': 'Colorado', '006': 'Connecticut', '007': 'Delaware', '008': 'Florida', '009': 'Georgia', '010': 'Idaho', '011': 'Illinois', '012': 'Indiana', '013': 'Iowa', '014': 'Kansas', '015': 'Kentucky', '016': 'Louisiana', '017': 'Maine', '018': 'Maryland', '019': 'Massachusetts', '020': 'Michigan', '021': 'Minnesota', '022': 'Mississippi', '023': 'Missouri', '024': 'Montana', '025': 'Nebraska', '026': 'Nevada', '027': 'New Hampshire', '028': 'New Jersey', '029': 'New Mexico', '030': 'New York', '031': 'North Carolina', '032': 'North Dakota', '033': 'Ohio', '034': 'Oklahoma', '035': 'Oregon', '036': 'Pennsylvania', '037': 'Rhode Island', '038': 'South Carolina', '039': 'South Dakota', '040': 'Tennessee', '041': 'Texas', '042': 'Utah', '043': 'Vermont', '044': 'Virginia', '045': 'Washington', '046': 'West Virginia', '047': 'Wisconsin', '048': 'Wyoming', '050': 'Alaska', '101': 'Northeast Region', '102': 'East North Central Region', '103': 'Central Region', '104': 'Southeast Region', '105': 'West North Central Region', '106': 'South Region', '107': 'Southwest Region', '108': 'Northwest Region', '109': 'West Region', '110': 'National (contiguous 48 States)', '111': 'Great Plains', '115': 'Southern Plains and Gulf Coast', '120': 'US Rockies and Westward', '121': 'NWS Eastern Region', '122': 'NWS Southern Region', '123': 'NWS Central Region', '124': 'NWS Western Region', '201': 'Pacific Northwest Basin', '202': 'California River Basin', '203': 'Great Basin', '204': 'Lower Colorado River Basin', '205': 'Upper Colorado River Basin', '206': 'Rio Grande River Basin', '207': 'Texas Gulf Coast River Basin', '208': 'Arkansas-White-Red Basin', '209': 'Lower Mississippi River Basin', '210': 'Missouri River Basin', '211': 'Souris-Red-Rainy Basin', '212': 'Upper Mississippi River Basin', '213': 'Great Lakes Basin', '214': 'Tennessee River Basin', '215': 'Ohio River Basin', '216': 'South Atlantic-Gulf Basin', '217': 'Mid-Atlantic Basin', '218': 'New England Basin', '220': 'Mississippi River Basin & Tributaties (N. of Memphis, TN)', '250': 'Spring Wheat Belt (area weighted)', '255': 'Primary Hard Red Winter Wheat Belt (area weighted)', '256': 'Winter Wheat Belt (area weighted)', '260': 'Primary Corn and Soybean Belt (area weighted)', '261': 'Corn Belt (area weighted)', '262': 'Soybean Belt (area weighted)', '265': 'Cotton Belt (area weighted)', '350': 'Spring Wheat Belt (productivity weighted)', '356': 'Winter Wheat Belt (productivity weighted)', '361': 'Corn Belt (productivity weighted)', '362': 'Soybean Belt (productivity weighted)', '365': 'Cotton Belt (productivity weighted)', '450': 'Spring Wheat Belt (% productivity in the Palmer Z Index)', '456': 'Winter Wheat Belt (% productivity in the Palmer Z Index)', '461': 'Corn Belt (% productivity in the Palmer Z Index)', '462': 'Soybean Belt (% productivity in the Palmer Z Index)', '465': 'Cotton Belt (% productivity in the Palmer Z Index)'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add column *state* to the dataset using the dictionary"
      ],
      "metadata": {
        "id": "PmXcRcHL4DM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_dataset['state'] = max_dataset.apply(lambda row: code_to_state[row['state_code']]\n",
        "                                         if row['state_code'] in code_to_state\n",
        "                                         else np.nan, axis=1)\n",
        "\n",
        "min_dataset['state'] = min_dataset.apply(lambda row: code_to_state[row['state_code']]\n",
        "                                         if row['state_code'] in code_to_state\n",
        "                                         else np.nan, axis=1)\n",
        "\n",
        "avg_dataset['state'] = avg_dataset.apply(lambda row: code_to_state[row['state_code']]\n",
        "                                         if row['state_code'] in code_to_state\n",
        "                                         else np.nan, axis=1)"
      ],
      "metadata": {
        "id": "p06CxjQazff_"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop useless columns"
      ],
      "metadata": {
        "id": "0luWpyHe3dGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_dataset.drop(columns = ['Code', 'state_code'], inplace = True)\n",
        "min_dataset.drop(columns = ['Code', 'state_code'], inplace = True)\n",
        "avg_dataset.drop(columns = ['Code', 'state_code'], inplace = True)"
      ],
      "metadata": {
        "id": "lc9k5fBg1ipN"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export the .csv files"
      ],
      "metadata": {
        "id": "iPwNnVKoym7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_csv(csv_name, dataset):\n",
        "\n",
        "  '''\n",
        "  csv_name = output filename\n",
        "  dataset = temperature dataset from which data has to be read row by row\n",
        "  '''\n",
        "\n",
        "  with open(csv_name, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    header = ['state', 'year', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'JanF', 'FebF', 'MarF', 'AprF', 'MayF', 'JunF', 'JulF', 'AugF', 'SepF', 'OctF', 'NovF', 'DecF']\n",
        "    writer.writerow(header)\n",
        "    for idx in range(dataset.shape[0]):\n",
        "      # row with: state, year, 12 months temperatures in C, 12 months temperatures in F\n",
        "      row = [dataset.iloc[idx, 25]] + [int(dataset.iloc[idx, 24])] + list(dataset.iloc[idx, 12:24]) + list(dataset.iloc[idx, :12])\n",
        "      writer.writerow(row)"
      ],
      "metadata": {
        "id": "taQx980jE8D0"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_list = list(code_to_state.values())[:49]\n",
        "\n",
        "# Create .csv files with MAX temperatures for each state\n",
        "for state in state_list:\n",
        "  csv_name = str(state.replace(\" \", \"\")) + 'MAX.csv'\n",
        "  dataset = max_dataset[max_dataset['state'] == state]\n",
        "  create_csv(csv_name, dataset)\n",
        "\n",
        "# Create .csv files with MIN temperatures for each state\n",
        "for state in state_list:\n",
        "  csv_name = str(state.replace(\" \", \"\")) + 'MIN.csv'\n",
        "  dataset = min_dataset[min_dataset['state'] == state]\n",
        "  create_csv(csv_name, dataset)\n",
        "\n",
        "# Create .csv files with AVG temperatures for each state\n",
        "for state in state_list:\n",
        "  csv_name = str(state.replace(\" \", \"\")) + 'AVG.csv'\n",
        "  dataset = avg_dataset[avg_dataset['state'] == state]\n",
        "  create_csv(csv_name, dataset)"
      ],
      "metadata": {
        "id": "KyeygW1VDnrm"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load tree_dataset"
      ],
      "metadata": {
        "id": "wduOcyh_2j2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment the following line of code to load the already pre-processed *tree_dataset* from google drive"
      ],
      "metadata": {
        "id": "TikljYvEHD6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tree_dataset = pd.read_csv(\"/content/drive/MyDrive/tree_dataset/final_dataset/tree_dataset.csv\", low_memory=False)"
      ],
      "metadata": {
        "id": "-yb7Zoj7Uwl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download ALL .csv files"
      ],
      "metadata": {
        "id": "Xu0I1XPBTWqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content'\n",
        "file_type = \".csv\"\n",
        "files_to_download = glob.glob(f\"{directory_path}/*{file_type}\")\n",
        "zip_filename = \"datasets.zip\"\n",
        "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
        "    for file in files_to_download:\n",
        "        zipf.write(file, arcname=os.path.basename(file))\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "TCLdmxRCTUmf",
        "outputId": "8ce08fb9-b8c3-422e-bf3a-3085db9a5ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8597fff4-27ca-40ad-b4ca-69d9cf83b0c2\", \"datasets.zip\", 2531658)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}